{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIAG = \"Gracious Diagnosis\"\n",
    "\n",
    "import os\n",
    "import paramiko\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from paramiko import Ed25519Key\n",
    "from datetime import datetime\n",
    "import shutil\n",
    "from cmcrameri import cm\n",
    "import palettable\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "prettify_model_name = {\n",
    "    \"Llama-2-70B-chat-GPTQ\": \"Llama 2 Chat\",\n",
    "    \"Llama2-70B-OASST-SFT-v10-GPTQ\": \"OASST\",\n",
    "    \"WizardLM-70B-V1.0-GPTQ\": \"WizardLM\",\n",
    "    \"axiong_PMC_LLaMA_13B\": \"PMC Llama\",\n",
    "    \"ClinicalCamel-70B-GPTQ\": \"Clinical Camel\",\n",
    "    \"Meditron-70B-GPTQ\": \"Meditron\",\n",
    "    \"MIMIC Doctors\": \"MIMIC Doctors\"\n",
    "}\n",
    "\n",
    "bat_4_cmap = ListedColormap(palettable.scientific.sequential.Batlow_4.mpl_colors)\n",
    "davos_5_cmap = ListedColormap(palettable.scientific.sequential.Hawaii_5.mpl_colors)\n",
    "\n",
    "Bilbao_4_cmap = ListedColormap(palettable.scientific.sequential.Bilbao_4.mpl_colors)\n",
    "Devon_4_cmap = ListedColormap(palettable.scientific.sequential.Devon_4.mpl_colors)\n",
    "Tokyo_4_cmap = ListedColormap(palettable.scientific.sequential.Tokyo_4.mpl_colors)\n",
    "\n",
    "color_map = {\n",
    "    \"Appendicitis\" : davos_5_cmap.colors[0],\n",
    "    \"Cholecystitis\" : Devon_4_cmap.colors[1],\n",
    "    \"Diverticulitis\" : davos_5_cmap.colors[2],\n",
    "    \"Pancreatitis\" : davos_5_cmap.colors[3],\n",
    "    \"Llama 2 Chat\": bat_4_cmap.colors[0],\n",
    "    \"OASST\": bat_4_cmap.colors[1],\n",
    "    \"WizardLM\": bat_4_cmap.colors[2],\n",
    "    \"Clinical Camel\" : Tokyo_4_cmap.colors[1],\n",
    "    \"Meditron\" : Tokyo_4_cmap.colors[2],\n",
    "    \"MIMIC Doctors\": Bilbao_4_cmap.colors[-1],\n",
    "    \"Doctors\": Bilbao_4_cmap.colors[-2],\n",
    "    \"Mean\" : davos_5_cmap.colors[4]\n",
    "}\n",
    "\n",
    "\n",
    "from utils.logging import read_from_pickle_file\n",
    "\n",
    "from os.path import join\n",
    "from dataset.utils import load_hadm_from_file\n",
    "\n",
    "from evaluators.appendicitis_evaluator import AppendicitisEvaluator\n",
    "from evaluators.cholecystitis_evaluator import CholecystitisEvaluator\n",
    "from evaluators.diverticulitis_evaluator import DiverticulitisEvaluator\n",
    "from evaluators.pancreatitis_evaluator import PancreatitisEvaluator\n",
    "from run import load_evaluator\n",
    "from utils.nlp import latex_escape\n",
    "\n",
    "# Check new evaluation strategy\n",
    "def load_evaluator(pathology):\n",
    "    # Load desired evaluator\n",
    "    if pathology == \"appendicitis\":\n",
    "        evaluator = AppendicitisEvaluator()\n",
    "    elif pathology == \"cholecystitis\":\n",
    "        evaluator = CholecystitisEvaluator()\n",
    "    elif pathology == \"diverticulitis\":\n",
    "        evaluator = DiverticulitisEvaluator()\n",
    "    elif pathology == \"pancreatitis\":\n",
    "        evaluator = PancreatitisEvaluator()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return evaluator\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "def calculate_average(evals, field, pathology):\n",
    "  average = 0\n",
    "  for patient in evals.keys():\n",
    "    if field not in evals[patient]['scores']:\n",
    "        print(f\"{field} not in {patient}\")\n",
    "    average += evals[patient]['scores'][field]\n",
    "\n",
    "  average /= len(evals)\n",
    "  #print(f'{pathology}: {average:0.02} (n={len(evals)})'.rjust(30))\n",
    "  return average, len(evals)\n",
    "\n",
    "def calculate_percentages(evals, field):\n",
    "    max_field = field\n",
    "    if \"Late\" in field:\n",
    "        max_field = max_field.replace(\"Late \", \"\")\n",
    "    max_field = max_field.replace(\" Percentage\", \"\")\n",
    "    for patient in evals.keys():\n",
    "        evals[patient]['scores'][field] = evals[patient]['scores'][field[:-len(\" Percentage\")]] / evals[patient]['max_scores'][max_field]\n",
    "    return evals\n",
    "\n",
    "def count_unnecessary(evals, field):\n",
    "    for patient in evals.keys():\n",
    "        evals[patient]['scores'][field] = len(evals[patient]['answers'][field])\n",
    "    return evals\n",
    "\n",
    "\n",
    "def print_results(difficulty, all_evals):\n",
    "    id_difficulty = pickle.load(open('id_difficulty.pkl', 'rb'))\n",
    "    avg_scores = {}\n",
    "    avg_samples = {}\n",
    "    print(f'@@@ {difficulty} @@@'.center(30))\n",
    "    print()\n",
    "    if difficulty in ['easy', 'secondary', 'hard']:\n",
    "        all_evals_diff = {}\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "            all_evals_diff[patho] = {}\n",
    "            for _id in all_evals[patho].keys():\n",
    "                if _id in id_difficulty[patho][difficulty]:\n",
    "                    all_evals_diff[patho][_id] = all_evals[patho][_id]\n",
    "    else:\n",
    "        all_evals_diff = all_evals\n",
    "    for field in ['Diagnosis', 'Custom Parsings', 'Rounds', 'Physical Examination', 'Unnecessary Laboratory Tests', 'Unnecessary Imaging']:\n",
    "        avg_scores[field] = {}\n",
    "        avg_samples[field] = {}\n",
    "        print(f'### {field} ###'.center(30))\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "            if field in ['Unnecessary Laboratory Tests', 'Unnecessary Imaging']:\n",
    "                all_evals_diff[patho] = count_unnecessary(all_evals_diff[patho], field)\n",
    "\n",
    "            avg, n = calculate_average(all_evals_diff[patho], field, patho)\n",
    "            print(f'{patho}: {avg:0.02} (n={n})'.rjust(30))\n",
    "\n",
    "            avg_scores[field][patho] = avg\n",
    "            avg_samples[field][patho] = n\n",
    "        print(f'AVERAGE: {np.mean(list(avg_scores[field].values())):0.2} (n={round(np.mean(list(avg_samples[field].values())))})'.rjust(30))\n",
    "        print()\n",
    "\n",
    "# Check new evaluation strategy\n",
    "def load_evaluator(pathology):\n",
    "    # Load desired evaluator\n",
    "    if pathology == \"appendicitis\":\n",
    "        evaluator = AppendicitisEvaluator()\n",
    "    elif pathology == \"cholecystitis\":\n",
    "        evaluator = CholecystitisEvaluator()\n",
    "    elif pathology == \"diverticulitis\":\n",
    "        evaluator = DiverticulitisEvaluator()\n",
    "    elif pathology == \"pancreatitis\":\n",
    "        evaluator = PancreatitisEvaluator()\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    return evaluator\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def generate_latex_tables(model_scores, experiment_name):\n",
    "    model_dicts = list(model_scores.values())\n",
    "    model_names = list(model_scores.keys())\n",
    "    diseases = ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']\n",
    "    categories = list(model_dicts[0].keys())\n",
    "\n",
    "    model_names = [prettify_model_name[name] for name in model_names]\n",
    "    category_names  = [category.replace(\" \", \" \\\\\\\\ \") for category in categories]\n",
    "\n",
    "    # for mean performance\n",
    "    data = []\n",
    "    for model_dict, model_name in zip(model_dicts, model_names):\n",
    "        row = [model_name] + [sum(model_dict[category].values())/len(diseases) for category in categories]\n",
    "        data.append(row)\n",
    "    df = pd.DataFrame(data, columns=[\"Model\"] + [f\"\\\\thead{{{category}}}\" for category in category_names])\n",
    "    styler = df.style\n",
    "    styler.format({f\"\\\\thead{{{category}}}\": \"{:.2f}\" for category in category_names}).hide(level=0, axis=0)\n",
    "    print(\"\\\\begin{table}[ht]\")\n",
    "    print(\"\\\\begin{adjustwidth}{-1in}{-1in}\")\n",
    "    print(\"\\\\centering\")\n",
    "    if experiment_name:\n",
    "        experiment_name = f\" - {experiment_name}\"\n",
    "    print(f\"\\\\caption{{Mean model performance{experiment_name}}}\")\n",
    "    print(styler.to_latex(column_format='l' + 'c'*len(categories), hrules=True))\n",
    "    print(\"\\\\end{adjustwidth}\")\n",
    "    print(\"\\\\end{table}\\n\")\n",
    "\n",
    "    for disease in diseases:\n",
    "        data = []\n",
    "        for model_dict, model_name in zip(model_dicts, model_names):\n",
    "            row = [model_name] + [model_dict[category][disease] for category in categories]\n",
    "            data.append(row)\n",
    "        df = pd.DataFrame(data, columns=[\"Model\"] + [f\"\\\\thead{{{category}}}\" for category in category_names])\n",
    "        styler = df.style\n",
    "        styler.format({f\"\\\\thead{{{category}}}\": \"{:.2f}\" for category in category_names}).hide(level=0, axis=0)\n",
    "        print(\"\\\\begin{table}[ht]\")\n",
    "        print(\"\\\\begin{adjustwidth}{-1in}{-1in}\")\n",
    "        print(\"\\\\centering\")\n",
    "\n",
    "        print(f\"\\\\caption{{Model performance on {disease.capitalize()}{experiment_name}}}\")\n",
    "        print(styler.to_latex(column_format='l' + 'c'*len(categories), hrules=True))\n",
    "        print(\"\\\\end{adjustwidth}\")\n",
    "        print(\"\\\\end{table}\\n\")\n",
    "\n",
    "def generate_latex_tables_full_info(model_scores, experiment_name, bold_max=True, underline_second_max=True):\n",
    "    diseases = ['Mean', 'Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis']\n",
    "    models = list(model_scores.keys())\n",
    "\n",
    "    data = []\n",
    "    for model_name in models:\n",
    "        row = []\n",
    "        for disease in diseases:\n",
    "            row.append(model_scores[model_name][disease])\n",
    "        data.append(row)\n",
    "\n",
    "    df = pd.DataFrame(data, columns=diseases, index=models)\n",
    "\n",
    "    if bold_max or underline_second_max:\n",
    "        def format_number(number, column, max_number, second_max_number):\n",
    "            if bold_max and number == max_number:\n",
    "                return \"\\\\textbf{%.2f}\" % number\n",
    "            elif underline_second_max and number == second_max_number:\n",
    "                return \"\\\\underline{%.2f}\" % number\n",
    "            else:\n",
    "                return \"%.2f\" % number\n",
    "\n",
    "        for column in df.columns:\n",
    "            max_number = df[column].max()\n",
    "            second_max_number = df[df[column] != max_number][column].max() if underline_second_max else None\n",
    "            df[column] = df[column].apply(lambda num: format_number(num, column, max_number, second_max_number))\n",
    "\n",
    "    latex_content = df.to_latex(escape=False, multirow=True, column_format=\"|c\"* (len(diseases) + 1) + \"|\")\n",
    "\n",
    "    for disease in diseases:\n",
    "        latex_content = latex_content.replace(f'\\\\multicolumn{{2}}{{r}}{{{disease}}}', f'\\\\multicolumn{{2}}{{|c|}}{{\\\\textbf{{{disease}}}}}')\n",
    "\n",
    "    print(\"\\\\begin{table}[ht]\")\n",
    "    print(\"\\\\centering\")\n",
    "    if experiment_name:\n",
    "        experiment_name = f\" - {latex_escape(experiment_name)}\"\n",
    "    print(f\"\\\\caption{{Diagnostic Accuracy (\\%) with Full Information{experiment_name}}}\")\n",
    "    print(latex_content)\n",
    "    print(\"\\\\end{table}\\n\")\n",
    "\n",
    "def load_scores(experiments, difficulty=\"first_diag\", fields=['Diagnosis'], models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"], pathos = ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']):\n",
    "    id_difficulty = pickle.load(open('id_difficulty.pkl', 'rb'))\n",
    "\n",
    "    experiment_results = {}\n",
    "    experiment_evals = {}\n",
    "    experiment_scores = {}\n",
    "    for experiment in experiments:\n",
    "        model_results = {}\n",
    "        model_evals = {}\n",
    "        model_scores = {}\n",
    "        for model in models:\n",
    "            m_results = pickle.load(open(f'logs/SOTA/{experiment}/{model}_results.pkl', 'rb'))\n",
    "            m_evals = pickle.load(open(f'logs/SOTA/{experiment}/{model}_evals.pkl', 'rb'))\n",
    "            model_results[model] = m_results\n",
    "            model_evals[model] = m_evals\n",
    "\n",
    "        for model in models:\n",
    "            all_evals = {}\n",
    "            all_results = {}\n",
    "            for patho in pathos:\n",
    "                all_evals[patho] = model_evals[model][patho]\n",
    "                all_results[patho] = model_results[model][patho]\n",
    "                \n",
    "                # Subset to only desired ids\n",
    "                selected_evals = {}\n",
    "                selected_results = {}\n",
    "                for _id in id_difficulty[patho][difficulty]:\n",
    "                    if _id not in all_evals[patho]:\n",
    "                        # Manually tested and all were correct\n",
    "                        if _id == 21285450:\n",
    "                            selected_evals[_id] = {'scores': {'Diagnosis': 1.0, \"Gracious Diagnosis\": 1.0, \"Action Parsing\": 0, \"Treatment Parsing\": 0, \"Diagnosis Parsing\": 0, 'Invalid Tools': 0}}\n",
    "                        else:\n",
    "                            print(f\"For experiment {experiment} and model {model}, {_id} not in {patho}\")\n",
    "                        continue\n",
    "                    selected_evals[_id] = all_evals[patho][_id]\n",
    "                    selected_results[_id] = all_results[patho][_id]\n",
    "                all_evals[patho] = selected_evals\n",
    "                all_results[patho] = selected_results\n",
    "            \n",
    "            model_evals[model] = all_evals\n",
    "            model_results[model] = all_results\n",
    "            avg_scores = {}\n",
    "            avg_samples = {}\n",
    "\n",
    "            field = 'Diagnosis'\n",
    "            avg_scores[field] = {}\n",
    "            avg_samples[field] = {}\n",
    "            for field in fields:\n",
    "                avg_scores[field] = {}\n",
    "                avg_samples[field] = {}\n",
    "                for patho in pathos:\n",
    "                    if field in ['Unnecessary Laboratory Tests', 'Unnecessary Imaging']:\n",
    "                        all_evals[patho] = count_unnecessary(all_evals[patho], field)\n",
    "                    avg, n = calculate_average(all_evals[patho], field, patho)\n",
    "\n",
    "                    avg_scores[field][patho] = avg\n",
    "                    avg_samples[field][patho] = n\n",
    "            model_scores[model] = avg_scores\n",
    "        experiment_results[experiment] = model_results\n",
    "        experiment_evals[experiment] = model_evals\n",
    "        experiment_scores[experiment] = model_scores\n",
    "    return experiment_results, experiment_evals, experiment_scores\n",
    "\n",
    "def check_diagnoses_orig_dr_eval(ids, id_difficulty):\n",
    "    for patho in ['appendicitis', 'cholecystitis', 'pancreatitis', 'diverticulitis']:\n",
    "        id_difficulty[patho]['original_dr_eval'] = []\n",
    "    for i in ids:\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'pancreatitis', 'diverticulitis']:\n",
    "            if i in id_difficulty[patho]['first_diag']:\n",
    "                print(f\"{i} is in {patho}\")\n",
    "                id_difficulty[patho]['original_dr_eval'].append(i)\n",
    "    for patho in \"gastritis\", \"urinary_tract_infection\", \"hernia\", \"esophageal_reflux\":\n",
    "        id_difficulty[patho]['original_dr_eval'] = id_difficulty[patho]['dr_eval']\n",
    "    return id_difficulty\n",
    "\n",
    "def check_diagnoses(ids, id_difficulty):\n",
    "    for i in ids:\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'pancreatitis', 'diverticulitis']:\n",
    "            if i in id_difficulty[patho]['first_diag']:\n",
    "                print(f\"{i} is in {patho}\")\n",
    "\n",
    "def print_diagnoses(difficulty):\n",
    "    id_difficulty = pickle.load(open('id_difficulty.pkl', 'rb'))\n",
    "    diags = {}\n",
    "    for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "        for _id in id_difficulty[patho][difficulty]:\n",
    "            diags[_id] = patho\n",
    "\n",
    "    diag_keys = list(diags.keys())\n",
    "    diag_keys.sort()\n",
    "    for key in diag_keys:\n",
    "        print(f\"{key} {diags[key]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CDM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate LateX table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"CDM_VANILLA\"\n",
    "#models = [\"axiong_PMC_LLaMA_13B\"]\n",
    "fields = ['Diagnosis', 'Physical Examination', 'Action Parsing', 'Treatment Parsing', 'Diagnosis Parsing', 'Rounds', 'Invalid Tools', 'Unnecessary Laboratory Tests', 'Unnecessary Imaging']\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields)\n",
    "model_scores = experiment_scores[experiment]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_latex_tables(model_scores, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Diagnostic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiment = 'CDM_VANILLA'\n",
    "fields = [DIAG]\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[experiment]\n",
    "\n",
    "data = []\n",
    "for model in model_scores.keys():\n",
    "    mean_diagnosis = np.mean([model_scores[model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "        data.append([model, patho.capitalize(), model_scores[model][DIAG][patho]])\n",
    "    data.append([model, 'Mean', mean_diagnosis])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Pathology', 'Diagnostic Accuracy'])\n",
    "\n",
    "# Reshaping the dataframe\n",
    "#melted_df = df.melt(id_vars=['Model'], var_name='Category', value_name='Diagnostic Accuracy' )\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "df['Model'] = df['Model'].apply(lambda x: prettify_model_name[x])\n",
    "df['Diagnostic Accuracy'] *= 100\n",
    "bar_plot = sns.barplot(x='Pathology', y='Diagnostic Accuracy', hue='Model', data=df, palette=color_map)\n",
    "\n",
    "unique_patho = df['Pathology'].unique()\n",
    "for i in range(len(unique_patho) - 1):\n",
    "    bar_plot.axvline(x=i + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.1f'), \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha = 'center', va = 'center', \n",
    "                      xytext = (0, 9), \n",
    "                      textcoords = 'offset points',\n",
    "                      fontsize=14,)\n",
    "\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('')\n",
    "plt.ylabel('Diagnostic Accuracy (%)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(bbox_to_anchor=(0.8, 1.18),  ncol=len(model_scores.keys()), frameon=False, fontsize=15)\n",
    "plt.savefig(f\"Figures/DiagnosticAccuraciesCDM.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Imaging Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from dataset.utils import load_hadm_from_file\n",
    "\n",
    "experiment = 'CDM_VANILLA'\n",
    "fields = [DIAG]\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[experiment]\n",
    "model_results = experiment_results[experiment]\n",
    "\n",
    "modality_map = {\n",
    "    'CT': 'CT', \n",
    "    'Ultrasound': 'Ultrasound', \n",
    "    'Radiograph': 'Radiograph',\n",
    "    'MRI': 'MRI',\n",
    "    'CTU': 'CT', \n",
    "    'ERCP': 'Radiograph', \n",
    "    'Upper GI Series': 'Radiograph',\n",
    "    'MRE': 'MRI', \n",
    "    'MRCP': 'MRI',\n",
    "    'MRA': 'MRI',\n",
    "    'EUS': 'Ultrasound',\n",
    "    'Carotid ultrasound': 'Ultrasound',\n",
    "    'None': 'None', \n",
    "    'PTC': 'Other', \n",
    "    'HIDA': 'Other', \n",
    "    'Drainage': 'Other', \n",
    "}\n",
    "\n",
    "data = []\n",
    "for model in model_results.keys():\n",
    "    for patho in model_results[model].keys():\n",
    "        for _id in model_results[model][patho].keys():\n",
    "            imaging_requested = False\n",
    "            for step in model_results[model][patho][_id]['intermediate_steps']:\n",
    "                if step[0].tool == 'Imaging':\n",
    "                    tool_input = step[0].tool_input\n",
    "                    region = tool_input['action_input'][\"region\"]\n",
    "                    modality = tool_input['action_input'][\"modality\"]\n",
    "                    if region == \"Abdomen\":\n",
    "                        imaging_requested = True\n",
    "                        modality = modality_map[modality]\n",
    "                        data.append({\"Model\": prettify_model_name[model], \"Pathology\": patho.capitalize(), \"Modality\": modality})\n",
    "                        break\n",
    "            if not imaging_requested:\n",
    "                data.append({\"Model\": prettify_model_name[model], \"Pathology\": patho.capitalize(), \"Modality\": \"None\"})\n",
    "\n",
    "app_hadm_info_firstdiag         = load_hadm_from_file('appendicitis_hadm_info_first_diag')\n",
    "cholec_hadm_info_firstdiag      = load_hadm_from_file('cholecystitis_hadm_info_first_diag')\n",
    "pancr_hadm_info_firstdiag       = load_hadm_from_file('pancreatitis_hadm_info_first_diag')\n",
    "divert_hadm_info_firstdiag      = load_hadm_from_file('diverticulitis_hadm_info_first_diag')\n",
    "\n",
    "for patho, hadm_info in zip(['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis'], [app_hadm_info_firstdiag, cholec_hadm_info_firstdiag, divert_hadm_info_firstdiag, pancr_hadm_info_firstdiag]):\n",
    "    for patient in hadm_info.keys():\n",
    "        for rad in hadm_info[patient][\"Radiology\"]:\n",
    "            if rad[\"Region\"] == \"Abdomen\":\n",
    "                #data.append({\"Model\": \"Dataset\", \"Pathology\": patho.capitalize(), \"Modality\": rad[\"Modality\"]})\n",
    "                data.append({\"Model\": \"MIMIC Doctors\", \"Pathology\": patho.capitalize(), \"Modality\": modality_map[rad[\"Modality\"]]})\n",
    "                break\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Pathology', 'Modality'])\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# 1. Calculating Counts\n",
    "counts = df.groupby(['Model', 'Pathology', 'Modality']).size().reset_index(name='Counts')\n",
    "\n",
    "# 2. Calculating Total Counts for Each Pathology and Model\n",
    "total_counts = counts.groupby(['Model', 'Pathology'])['Counts'].transform('sum')\n",
    "\n",
    "# 3. Calculating Percentages\n",
    "counts['Percentage'] = (counts['Counts'] / total_counts) * 100\n",
    "\n",
    "# Preparing data for grouped bar chart\n",
    "models = counts['Model'].unique()\n",
    "pathologies = counts['Pathology'].unique()\n",
    "modalities = counts['Modality'].unique()\n",
    "\n",
    "# Number of bars (models) per group (pathology)\n",
    "n_bars = len(models)\n",
    "\n",
    "# Width of each bar\n",
    "bar_width = 0.2\n",
    "\n",
    "# Define the custom order for modalities\n",
    "modality_order = [\"CT\", \"Ultrasound\", \"MRI\", \"Radiograph\", \"Other\", \"None\"]\n",
    "\n",
    "# Defining a color map for modalities\n",
    "modality_colors = {modality: plt.cm.Accent(i/len(modality_order)) for i, modality in enumerate(modality_order)}\n",
    "\n",
    "# Defining model order\n",
    "model_order = [\"Llama 2 Chat\", \"OASST\", \"WizardLM\", \"MIMIC Doctors\"]\n",
    "\n",
    "# Define hatch patterns for models\n",
    "model_hatches = ['/', '.', '-', \"x\"]\n",
    "light_edge_colors = ['lightgrey']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "for j, model in enumerate(model_order):\n",
    "    offsets = np.arange(len(pathologies)) + (j - n_bars / 2) * bar_width\n",
    "    bottom_val = np.zeros(len(pathologies))\n",
    "\n",
    "    for modality in modality_order:\n",
    "        model_modality_data = counts[(counts['Model'] == model) & (counts['Modality'] == modality)]\n",
    "        heights = model_modality_data.set_index('Pathology').reindex(pathologies)['Percentage'].fillna(0).tolist()\n",
    "\n",
    "        # Adding the bars\n",
    "        bars = plt.bar(offsets, heights, width=bar_width, bottom=bottom_val, color=modality_colors[modality], hatch=model_hatches[j], edgecolor=\"lightgrey\", label=f\"{modality}\" if j == 0 else \"\")\n",
    "\n",
    "        for bar in bars:\n",
    "            height = bar.get_height()\n",
    "            if height > 5:  # Only add text labels if the bar segment is visible\n",
    "                y_position = bar.get_y() + height / 2  # Calculate the y position for the label\n",
    "                text_color = 'white' if (modality==\"Radiograph\" or modality==\"None\") else 'black'  # Choose text color for readability\n",
    "                plt.text(bar.get_x() + bar.get_width() / 2, y_position, f'{height:.0f}', ha='center', va='center', color=text_color)\n",
    "\n",
    "\n",
    "        bottom_val += np.array(heights)\n",
    "\n",
    "plt.xlabel('Pathology')\n",
    "plt.ylabel('Imaging Modality Requested (%)')\n",
    "#plt.title('Percentage of Modalities per Pathology by Model')\n",
    "plt.xticks(np.arange(len(pathologies))-0.1, pathologies)\n",
    "\n",
    "# Adjusting legend to show modalities only\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "by_label = dict(zip(labels, handles))\n",
    "plt.legend(by_label.values(), by_label.keys(), bbox_to_anchor=(0.9, 1.1),  ncol=len(by_label.keys()), frameon=False, fontsize=16)\n",
    "\n",
    "# Create custom legend handles for modalities\n",
    "modality_handles = [mpatches.Patch(color=modality_colors[modality], label=modality) for modality in modality_order]\n",
    "\n",
    "# Create custom legend handles for models using hatch patterns\n",
    "dense_model_hatches = ['///', '..', '---', 'xxx']\n",
    "model_handles = [mpatches.Patch(facecolor='white', edgecolor='black', hatch=dense_model_hatches[j], label=model_order[j]) for j in range(len(models))]\n",
    "\n",
    "# Combine the modality and model handles\n",
    "legend_handles = modality_handles + model_handles\n",
    "\n",
    "# Add the legend to the plot\n",
    "plt.legend(handles=legend_handles, bbox_to_anchor=(0.95, 1.25), ncol=5, frameon=False, fontsize=15)\n",
    "\n",
    "\n",
    "plt.savefig(f\"Figures/ImagingPercentages.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Physical Examination Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiment = 'CDM_VANILLA'\n",
    "fields = ['Physical Examination', 'Late Physical Examination']\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[experiment]\n",
    "model_results = experiment_results[experiment]\n",
    "model_evals = experiment_evals[experiment]\n",
    "\n",
    "data = []\n",
    "for model in model_scores.keys():\n",
    "    mean_physical = np.mean([model_scores[model]['Physical Examination'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    mean_physical_late = np.mean([model_scores[model]['Late Physical Examination'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    data.append([model, mean_physical, mean_physical_late])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Physical Examination', '(Late) Physical Examination'])\n",
    "df['Physical Examination'] *= 100\n",
    "df['(Late) Physical Examination'] *= 100\n",
    "\n",
    "# Reshaping the dataframe\n",
    "melted_df = df.melt(id_vars=['Model'], var_name='Category', value_name='Percentage' )\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(10, 4))\n",
    "melted_df['Model'] = melted_df['Model'].apply(lambda x: prettify_model_name[x])\n",
    "\n",
    "bar_plot = sns.barplot(x='Category', y='Percentage', hue='Model', data=melted_df, palette=color_map)\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.1f'), \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha = 'center', va = 'center', \n",
    "                      xytext = (0, 9), \n",
    "                      textcoords = 'offset points')\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('')\n",
    "plt.ylabel('Examination Requested (%)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(bbox_to_anchor=(0.9, 1.2),  ncol=len(model_scores.keys()), frameon=False, fontsize=16)\n",
    "plt.savefig(f\"Figures/PhysicalExaminationPercentages.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Lab Test Percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.AgentAction import AgentAction\n",
    "from dataset.utils import load_hadm_from_file\n",
    "\n",
    "from evaluators.appendicitis_evaluator import AppendicitisEvaluator\n",
    "from evaluators.cholecystitis_evaluator import CholecystitisEvaluator\n",
    "from evaluators.diverticulitis_evaluator import DiverticulitisEvaluator\n",
    "from evaluators.pancreatitis_evaluator import PancreatitisEvaluator\n",
    "\n",
    "evaluators = {\n",
    "    'appendicitis': AppendicitisEvaluator(),\n",
    "    'cholecystitis': CholecystitisEvaluator(),\n",
    "    'diverticulitis': DiverticulitisEvaluator(),\n",
    "    'pancreatitis': PancreatitisEvaluator(),\n",
    "}\n",
    "\n",
    "experiment = 'CDM_VANILLA'\n",
    "fields = []\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[experiment]\n",
    "model_results = experiment_results[experiment]\n",
    "model_evals = experiment_evals[experiment]\n",
    "\n",
    "required_lab_tests = {}\n",
    "for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "    required_lab_tests[patho] = evaluators[patho].required_lab_tests\n",
    "\n",
    "app_hadm_info_firstdiag         = load_hadm_from_file('appendicitis_hadm_info_first_diag')\n",
    "cholec_hadm_info_firstdiag      = load_hadm_from_file('cholecystitis_hadm_info_first_diag')\n",
    "pancr_hadm_info_firstdiag       = load_hadm_from_file('pancreatitis_hadm_info_first_diag')\n",
    "divert_hadm_info_firstdiag      = load_hadm_from_file('diverticulitis_hadm_info_first_diag')\n",
    "\n",
    "model_evals[\"MIMIC Doctors\"] = {}\n",
    "for patho, hadm_info in zip(['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis'], [app_hadm_info_firstdiag, cholec_hadm_info_firstdiag, divert_hadm_info_firstdiag, pancr_hadm_info_firstdiag]):\n",
    "    model_evals[\"MIMIC Doctors\"][patho] = {}\n",
    "    for patient in hadm_info.keys():\n",
    "        evaluator = evaluators[patho]\n",
    "        action = AgentAction(tool = \"Laboratory Tests\", tool_input = {\"action_input\" : list(hadm_info[patient]['Laboratory Tests'].keys())}, log = \"\", custom_parsings=0)\n",
    "        evaluator.score_laboratory_tests(action)\n",
    "        eval = {\n",
    "            \"scores\": evaluator.scores,\n",
    "            \"answers\": evaluator.answers,\n",
    "        }\n",
    "        model_evals[\"MIMIC Doctors\"][patho][patient] = eval\n",
    "\n",
    "data = []\n",
    "required_lab_test_percentages = {}\n",
    "for model in model_evals.keys():\n",
    "    required_lab_test_percentages[model] = {}\n",
    "    for patho in model_evals[model].keys():\n",
    "        required_lab_test_percentages[model][patho] = {}\n",
    "        for required_lab_test in required_lab_tests[patho]:\n",
    "            required_lab_test_percentages[model][patho][required_lab_test] = 0\n",
    "        for _id in model_evals[model][patho].keys():\n",
    "            for required_lab_test in required_lab_tests[patho]:\n",
    "                required_lab_test_percentages[model][patho][required_lab_test] += 1 if len(model_evals[model][patho][_id]['answers']['Correct Laboratory Tests'][required_lab_test]) else 0\n",
    "        for required_lab_test in required_lab_tests[patho]:\n",
    "            required_lab_test_percentages[model][patho][required_lab_test] /= len(model_evals[model][patho].keys()) \n",
    "            required_lab_test_percentages[model][patho][required_lab_test] *= 100\n",
    "            data.append([prettify_model_name[model], patho.capitalize(), required_lab_test, required_lab_test_percentages[model][patho][required_lab_test]])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Pathology', 'Required Laboratory Test', 'Percentage'])\n",
    "\n",
    "# Creating the plot\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.6)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10), sharey=True)\n",
    "\n",
    "# Replace seriousness with severity\n",
    "df['Required Laboratory Test'] = df['Required Laboratory Test'].replace('Seriousness', 'Severity')\n",
    "\n",
    "# Pathologies and their relevant tests\n",
    "pathology_tests = {\n",
    "    'Appendicitis': ['Inflammation'],\n",
    "    'Cholecystitis': ['Inflammation', 'Liver', 'Gallbladder'],\n",
    "    'Diverticulitis': ['Inflammation'],\n",
    "    'Pancreatitis': ['Inflammation', 'Pancreas', 'Severity']\n",
    "}\n",
    "\n",
    "# Plotting each pathology in its subplot\n",
    "for i, (pathology, tests) in enumerate(pathology_tests.items()):\n",
    "    print(i, pathology)\n",
    "    ax = axes[i // 2, i % 2]\n",
    "    pathology_df = df[(df['Pathology'] == pathology) & \n",
    "                               (df['Required Laboratory Test'].isin(tests))]\n",
    "    bar_plot = sns.barplot(x='Required Laboratory Test', y='Percentage', hue='Model', data=pathology_df, ax=ax, palette=color_map)\n",
    "\n",
    "    unique_patho = pathology_df['Required Laboratory Test'].unique()\n",
    "    for j in range(len(unique_patho) - 1):\n",
    "        bar_plot.axvline(x=j + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "    ax.set_title(pathology)\n",
    "    if i > 1:\n",
    "        ax.set_xlabel('Laboratory Test Category')\n",
    "    else:\n",
    "        ax.set_xlabel('')\n",
    "    ax.set_ylabel('Lab Test Requested (%)')\n",
    "    bar_plot.get_legend().remove()\n",
    "\n",
    "    # Print % correct above each bar\n",
    "    for p in bar_plot.patches:\n",
    "        if p.get_height() > 0:\n",
    "            adjust = 0 if p.get_height() < 99 else 10\n",
    "            color = 'black' if p.get_height() < 99 else 'white'\n",
    "            _format = '.1f' if p.get_height() < 99 else '.0f'\n",
    "            bar_plot.annotate(format(p.get_height(), _format), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height() - adjust), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points',\n",
    "                fontsize=15, color=color)\n",
    "\n",
    "# Adjust layout for clarity\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "plt.legend(bbox_to_anchor=(0.7, 2.57),  ncol=len(model_evals.keys()), frameon=False, fontsize=20)\n",
    "plt.savefig(f\"Figures/LaboratoryTestPercentages.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Instruction Following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiments = ['CDM_VANILLA', 'FI_PLI']\n",
    "fields = ['Action Parsing', 'Treatment Parsing', 'Diagnosis Parsing', 'Invalid Tools']\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[\"CDM_VANILLA\"]\n",
    "model_results = experiment_results[\"CDM_VANILLA\"]\n",
    "model_evals = experiment_evals[\"CDM_VANILLA\"]\n",
    "\n",
    "model_scores_fi = experiment_scores[\"FI_PLI\"]\n",
    "model_results_fi = experiment_results[\"FI_PLI\"]\n",
    "model_evals_fi = experiment_evals[\"FI_PLI\"]\n",
    "\n",
    "data = []\n",
    "for model in model_scores.keys():\n",
    "    mean_action_parsing = np.mean([model_scores[model]['Action Parsing'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    mean_treatment_parsing = np.mean([model_scores[model]['Treatment Parsing'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    mean_diagnosis_parsing = np.mean([model_scores[model]['Diagnosis Parsing'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    mean_diagnosis_parsing_fi = np.mean([model_scores_fi[model]['Diagnosis Parsing'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    mean_invalid_tools = np.mean([model_scores[model]['Invalid Tools'][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    data.append([model, mean_action_parsing, mean_diagnosis_parsing, mean_invalid_tools, mean_diagnosis_parsing_fi])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Action Parsing', 'Diagnosis Parsing', 'Invalid Tools', 'Diagnosis Parsing FI'])\n",
    "df[\"Model\"] = df[\"Model\"].apply(lambda x: prettify_model_name[x])\n",
    "df['Invalid Tools'] = 1 / df['Invalid Tools']\n",
    "df[\"Action Parsing\"] = 1 / df[\"Action Parsing\"]\n",
    "#df[\"Defining Treatment\"] = 1 / df[\"Defining Treatment\"]\n",
    "df[\"Diagnosis Parsing\"] = 1 / df[\"Diagnosis Parsing\"]\n",
    "df[\"Diagnosis Parsing FI\"] = 1 / df[\"Diagnosis Parsing FI\"]\n",
    "\n",
    "# Reshaping the dataframe\n",
    "melted_df = df.melt(id_vars=['Model'], var_name='Category', value_name='Percentage' )\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 4))\n",
    "bar_plot = sns.barplot(x='Category', y='Percentage', hue='Model', data=melted_df, palette=color_map, order=['Action Parsing', 'Invalid Tools', 'Diagnosis Parsing', 'Diagnosis Parsing FI'])\n",
    "\n",
    "unique_patho = melted_df['Category'].unique()\n",
    "for j in range(len(unique_patho) - 1):\n",
    "    bar_plot.axvline(x=j + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.2f'), \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha = 'center', va = 'center', \n",
    "                      xytext = (0, 9), \n",
    "                      textcoords = 'offset points')\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('')\n",
    "plt.ylabel('Average Number of Patients\\nUntil Formatting Error')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 30)\n",
    "plt.xticks(labels=['MIMIC-CDM\\nNext Action Error', 'MIMIC-CDM\\nTool Hallucination', 'MIMIC-CDM\\nDiagnosis Error', 'MIMIC-CDM-FI\\nDiagnosis Error',], ticks=[0, 1, 2, 3])\n",
    "plt.legend(bbox_to_anchor=(0.85, 1.2),  ncol=len(model_scores.keys()), frameon=False, fontsize=16)\n",
    "plt.savefig(f\"Figures/InstructionFollowingScores.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Treatment Percentage "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import FixedLocator\n",
    "\n",
    "experiment = 'CDM_VANILLA'\n",
    "fields = [DIAG]\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], fields=fields, models=models)\n",
    "\n",
    "model_scores = experiment_scores[experiment]\n",
    "model_evals = experiment_evals[experiment]\n",
    "\n",
    "def colo_replace(x):\n",
    "    if x == \"Colonoscopy\":\n",
    "        return \"Future\\nColonoscopy\"\n",
    "    else:\n",
    "        return x\n",
    "\n",
    "# Transform data structure into pandas DataFrame\n",
    "data = []\n",
    "treatment_required_counts = {}\n",
    "for model in model_evals:\n",
    "    for pathology in model_evals[model]:\n",
    "        treatment_required_counts[pathology] = {}\n",
    "        for patient_id in model_evals[model][pathology]:\n",
    "            treatment_requested = model_evals[model][pathology][patient_id]['answers']['Treatment Requested']\n",
    "            treatment_required = model_evals[model][pathology][patient_id]['answers']['Treatment Required']\n",
    "            correctly_diagnosed = model_evals[model][pathology][patient_id]['scores'][DIAG]\n",
    "\n",
    "            for treatment in treatment_required:\n",
    "                if treatment_required[treatment]:\n",
    "                    treatment_required_counts[pathology][treatment] = treatment_required_counts[pathology].get(treatment, 0) + 1\n",
    "                    if correctly_diagnosed:\n",
    "                        requested = treatment_requested.get(treatment, False)\n",
    "                        data.append([model, pathology, treatment, requested]) \n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Pathology', 'Treatment', 'Requested'])\n",
    "df['Model'] = df['Model'].replace(prettify_model_name)\n",
    "\n",
    "# Compute percentage of correctly requested treatment\n",
    "df_agg = df.groupby(['Model', 'Pathology', 'Treatment']).mean().reset_index()\n",
    "df_agg['Request Correct'] = df_agg['Requested']*100\n",
    "\n",
    "df_counts = df.groupby(['Pathology', 'Treatment', 'Model']).size().reset_index(name='Counts')\n",
    "\n",
    "# then merge this df_counts with your df_agg DataFrame\n",
    "df_agg = pd.merge(df_agg, df_counts,  how='left', left_on=['Pathology','Treatment', 'Model'], right_on = ['Pathology','Treatment', 'Model'])\n",
    "\n",
    "# Remove entries with count less than 5\n",
    "# df_agg = df_agg[df_agg['Counts'] >= 5]\n",
    "\n",
    "unique_pathologies = df_agg['Pathology'].unique()\n",
    "\n",
    "# Set the style and color palette\n",
    "sns.set_theme(style=\"whitegrid\", font_scale=1.8)\n",
    "\n",
    "# Create a 2x2 grid for subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(20, 12), sharey=True)\n",
    "\n",
    "# Flatten the axes array for easy iteration\n",
    "axes_flat = axes.flatten()\n",
    "\n",
    "# Iterate through each pathology and plot on its respective subplot\n",
    "for i, pathology in enumerate(unique_pathologies):\n",
    "    if i >= 4:  # Skip if there are more than 4 pathologies\n",
    "        break\n",
    "    df_filtered = df_agg[df_agg['Pathology'] == pathology]\n",
    "\n",
    "    bar_plot = sns.barplot(x='Treatment', y='Request Correct', hue='Model', data=df_filtered, ax=axes_flat[i], palette=color_map)\n",
    "\n",
    "    unique_patho = df_filtered['Treatment'].unique()\n",
    "    for j in range(len(unique_patho) - 1):\n",
    "        bar_plot.axvline(x=j + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "    # Print counts below each bar\n",
    "    for j, bar in enumerate(bar_plot.patches):\n",
    "        if j < len(df_filtered):\n",
    "            font_size = 16\n",
    "            bar_plot.text(bar.get_x() + bar.get_width() / 2, -6, \n",
    "                          '{}'.format(df_filtered['Counts'].iloc[j]), \n",
    "                          ha='center', va='bottom', fontsize=font_size)\n",
    "    \n",
    "    # Print % correct above each bar\n",
    "    for i,p in enumerate(bar_plot.patches):\n",
    "        if i < len(df_filtered):\n",
    "            bar_plot.annotate(format(p.get_height(), '.1f'), \n",
    "                (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                ha = 'center', va = 'center', \n",
    "                xytext = (0, 9), \n",
    "                textcoords = 'offset points',\n",
    "                fontsize=16)\n",
    "\n",
    "    # Set axes labels and titles\n",
    "    bar_plot.set_xlabel('')\n",
    "    bar_plot.set_ylabel('Treatment Requested (%)')\n",
    "    bar_plot.set_title(f'{pathology.capitalize()} Treatment')\n",
    "    label_with_count = lambda x: f\"\\n{colo_replace(x)}\\n(n={treatment_required_counts[pathology][x]})\"\n",
    "    tick_positions = range(len(df_filtered['Treatment'].unique()))\n",
    "    bar_plot.xaxis.set_major_locator(FixedLocator(tick_positions))\n",
    "    bar_plot.set_xticklabels([label_with_count(tick.get_text()) for tick in bar_plot.get_xticklabels()])\n",
    "    bar_plot.get_legend().remove()\n",
    "\n",
    "# Hide any unused subplots\n",
    "for ax in axes_flat[len(unique_pathologies):]:\n",
    "    ax.axis('off')\n",
    "\n",
    "# Set global parameters\n",
    "plt.ylim(0, 105)\n",
    "plt.setp(axes, xlabel='', ylabel='Treatment Requested (%)')\n",
    "\n",
    "# Create a single legend\n",
    "handles, labels = axes_flat[0].get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc='upper center', bbox_to_anchor=(0.5, 1.03), ncol=3, frameon=False)\n",
    "\n",
    "# Adjust layout and save the figure\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"Figures/TreatmentRequested_Combined.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual check of time component of colonoscopy and cholecystectomy\n",
    "model_evals = experiment_evals[\"CDM_VANILLA\"]\n",
    "for model in model_evals.keys():\n",
    "    print(f\"---\\n{model}\\n---\")\n",
    "    for p in model_evals[model]['diverticulitis']:\n",
    "        treatment = model_evals[model]['diverticulitis'][p]['answers']['Treatment']\n",
    "        if model_evals[model]['diverticulitis'][p]['scores']['Gracious Diagnosis'] == 1 and 'colonoscopy' in treatment.lower():\n",
    "            print(treatment)\n",
    "            print(\"--\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual check of time component of colonoscopy and cholecystectomy\n",
    "model_evals = experiment_evals[\"CDM_VANILLA\"]\n",
    "for model in model_evals.keys():\n",
    "    print(f\"---\\n{model}\\n---\")\n",
    "    for p in model_evals[model]['pancreatitis']:\n",
    "        treatment = model_evals[model]['pancreatitis'][p]['answers']['Treatment']\n",
    "        if model_evals[model]['pancreatitis'][p]['scores']['Gracious Diagnosis'] == 1 and 'cholecystectomy' in treatment.lower():\n",
    "            print(treatment)\n",
    "            print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization Effect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "fields = [DIAG]\n",
    "\n",
    "experiments = [\"CDM_VANILLA\", \"CDM_NOSUMMARY\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=fields, models=models)\n",
    "\n",
    "pathologies = ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']\n",
    "\n",
    "data_to_plot = []\n",
    "for experiment in experiment_scores.keys():\n",
    "    for model_idx, model_name in enumerate(models):\n",
    "        for pathology in pathologies:\n",
    "            data_to_plot.append({\n",
    "                'Experiment': experiment,\n",
    "                'Model': model_name,\n",
    "                'Pathology': pathology,\n",
    "                'Diagnostic Accuracy': experiment_scores[experiment][model_name][DIAG][pathology],\n",
    "            })\n",
    "        # Add mean\n",
    "        data_to_plot.append({\n",
    "            'Experiment': experiment,\n",
    "            'Model': model_name,\n",
    "            'Pathology': 'Mean',\n",
    "            'Diagnostic Accuracy': np.mean([experiment_scores[experiment][model_name][DIAG][pathology] for pathology in pathologies]),\n",
    "        })\n",
    "df_to_plot = pd.DataFrame(data_to_plot)\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "# Correctly calculating the differences\n",
    "df_filtered = df_to_plot[df_to_plot[\"Experiment\"].isin(experiments)]\n",
    "\n",
    "# First, we need to separate the data for each experiment\n",
    "df_vanilla = df_filtered[df_filtered[\"Experiment\"] == \"CDM_VANILLA\"]\n",
    "df_NOSUMMARY = df_filtered[df_filtered[\"Experiment\"] == \"CDM_NOSUMMARY\"]\n",
    "\n",
    "# Merging the two datasets on 'Model' and 'Pathology' to calculate differences\n",
    "df_merged = pd.merge(df_vanilla, df_NOSUMMARY, on=[\"Model\", \"Pathology\"], suffixes=('_VANILLA', '_NOSUMMARY'))\n",
    "\n",
    "# Calculating the differences\n",
    "df_merged['Diagnostic Accuracy'] = df_merged['Diagnostic Accuracy_NOSUMMARY'] - df_merged['Diagnostic Accuracy_VANILLA']\n",
    "\n",
    "# Selecting relevant columns for plotting\n",
    "df_differences = df_merged[['Model', 'Pathology', 'Diagnostic Accuracy']]\n",
    "\n",
    "# Display the first few rows of the dataframe with differences\n",
    "df_differences.head()\n",
    "\n",
    "# Capitalizing the pathology names\n",
    "df_differences['Pathology'] = df_differences['Pathology'].str.capitalize()\n",
    "\n",
    "df_differences['Model'] = df_differences['Model'].map(prettify_model_name)\n",
    "df_differences_mean = df_differences[df_differences['Pathology'] == 'Mean']\n",
    "df_differences_mean.drop(columns=['Pathology'], inplace=True)\n",
    "df_differences_mean = df_differences_mean.melt(id_vars=['Model'], var_name='Category', value_name='Percentage' )\n",
    "df_differences_mean['Percentage'] = 100*df_differences_mean['Percentage']\n",
    "\n",
    "# Setting the font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating a combined plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(12, 6))\n",
    "\n",
    "barplot = sns.barplot(x='Percentage', y=\"Model\", hue=\"Category\", data=df_differences_mean)\n",
    "barplot.set_xlabel(f\"Change in Mean Diagnostic Accuracy (%)\")\n",
    "\n",
    "# Adjusting layout and showing only one legend\n",
    "plt.tight_layout()\n",
    "\n",
    "#plt.legend(bbox_to_anchor=(1.0, 1.15),  ncol=5, frameon=False, fontsize=14)\n",
    "# remove legend\n",
    "barplot.get_legend().remove()\n",
    "\n",
    "plt.title(\"Effect of Removing Summarization During CDM\", fontsize=18)\n",
    "plt.savefig(f\"Figures/SummarizationCDMDelta.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Vanilla Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"FI_PLI\"\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment])\n",
    "model_scores = experiment_scores[experiment]\n",
    "\n",
    "# Replace keys with simplified names\n",
    "model_scores = {prettify_model_name[name]: model_scores[name] for name in model_scores.keys()}\n",
    "\n",
    "# Capitalize first letter of each disease\n",
    "model_scores = {model: {disease.capitalize(): score for disease, score in category_dict['Diagnosis'].items()} for model, category_dict in model_scores.items()}\n",
    "\n",
    "# Calculate mean performance and add to disease dict\n",
    "for category, disease_dict in model_scores.items():\n",
    "    disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment = \"\"\n",
    "generate_latex_tables_full_info(model_scores, experiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot Diagnostic Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiments = ['FI_PLI', 'FI_PLI_FEWSHOT']\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\", \"ClinicalCamel-70B-GPTQ\", \"Meditron-70B-GPTQ\"]\n",
    "fields = [DIAG]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, models=models, fields=fields)\n",
    "\n",
    "data = []\n",
    "for experiment in experiment_scores.keys():\n",
    "    model_scores = experiment_scores[experiment]\n",
    "    for model in model_scores.keys():\n",
    "        mean_diagnosis = np.mean([model_scores[model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "            data.append([experiment, model, patho.capitalize(), model_scores[model][DIAG][patho]])\n",
    "        data.append([experiment, model, 'Mean', mean_diagnosis])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Experiment', 'Model', 'Pathology', 'Diagnostic Accuracy'])\n",
    "df['Diagnostic Accuracy'] *= 100\n",
    "df['Model'] = df['Model'].apply(lambda x: prettify_model_name[x])\n",
    "\n",
    "# Extract best experiment of each model based on mean diagnostic accuracy\n",
    "best_experiments = {}\n",
    "for model in models:\n",
    "    model = prettify_model_name[model]\n",
    "    best_experiments[model] = df[(df['Model'] == model) & (df['Pathology']=='Mean')].sort_values(by=['Diagnostic Accuracy'], ascending=False).iloc[0]['Experiment']\n",
    "\n",
    "# For each model, remove rows that are not the best experiment\n",
    "df = df[df.apply(lambda x: x['Experiment'] == best_experiments[x['Model']], axis=1)]\n",
    "\n",
    "# Reshaping the dataframe\n",
    "#melted_df = df.melt(id_vars=['Model'], var_name='Category', value_name='Diagnostic Accuracy' )\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_plot = sns.barplot(x='Pathology', y='Diagnostic Accuracy', hue='Model', data=df, palette=color_map)\n",
    "\n",
    "# Calculate the number of unique pathologies and models\n",
    "num_pathologies = len(df['Pathology'].unique())\n",
    "num_models = len(df['Model'].unique())\n",
    "\n",
    "# Draw vertical dotted lines\n",
    "for i in range(num_pathologies - 1):\n",
    "    line_position = (i + 1) - 0.5\n",
    "    #plt.axvline(x=line_position, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.0f'), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points',\n",
    "                    fontsize=14,)\n",
    "\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('')\n",
    "plt.ylabel('Diagnostic Accuracy (%)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.13),  ncol=len(model_scores.keys()), frameon=False, fontsize=14)\n",
    "plt.savefig(f\"Figures/DiagnosticAccuraciesFI.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "    print(len(experiment_results['FI_PLI']['Llama-2-70B-chat-GPTQ'][patho]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dr Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_difficulty = pickle.load(open('id_difficulty.pkl', 'rb'))\n",
    "\n",
    "dr_eval_diags = {}\n",
    "for patho in id_difficulty.keys():\n",
    "    for _id in id_difficulty[patho]['dr_eval']:\n",
    "        dr_eval_diags[_id] = patho\n",
    "\n",
    "# read csv\n",
    "df = pd.DataFrame(columns=['Patient IDs', 'Category', 'Doctor', \"Correct Diagnosis\"])\n",
    "for dr in [\"Knauer\", \"Huber\", \"Vielhauer\", \"Bhagat\"]:\n",
    "    path = f\"ClinicalBenchmark/patient_diagnoses_{dr}.csv\"\n",
    "    dr_df = pd.read_csv(path, sep=';')\n",
    "    dr_df['Doctor'] = dr\n",
    "    df = pd.concat([df, dr_df])\n",
    "\n",
    "# Replace Divertikulitis with Diverticulitis\n",
    "df['Diagnosis'] = df['Category'].replace('Divertikulitis', 'Diverticulitis')\n",
    "df.drop(columns=['Category'], inplace=True)\n",
    "\n",
    "# Add correct diagnosis using id_difficulty entry\n",
    "df['Correct Diagnosis'] = df.apply(lambda row: dr_eval_diags[row['Patient IDs']].capitalize(), axis=1)\n",
    "\n",
    "accuracies = {}\n",
    "incorrect_ids = {}\n",
    "incorrect_diagoses = {}\n",
    "incorrect_dr = {}\n",
    "for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "    # Calculate accuracy \n",
    "    accuracies[patho.capitalize()] = df[df['Correct Diagnosis'] == patho.capitalize()]['Diagnosis'].eq(patho.capitalize()).sum() / len(df[df['Correct Diagnosis'] == patho.capitalize()]['Diagnosis'])\n",
    "\n",
    "    # Get diagnosis of incorrect predictions\n",
    "    incorrect = df[df['Correct Diagnosis'] == patho.capitalize()]['Diagnosis'].ne(patho.capitalize())\n",
    "    incorrect_ids[patho] = list(df[df['Correct Diagnosis'] == patho.capitalize()][incorrect]['Patient IDs'])\n",
    "    incorrect_diagoses[patho] = list(df[df['Correct Diagnosis'] == patho.capitalize()][incorrect]['Diagnosis'])\n",
    "    incorrect_dr[patho] = list(df[df['Correct Diagnosis'] == patho.capitalize()][incorrect]['Doctor'])\n",
    "\n",
    "data_to_plot = []\n",
    "\n",
    "experiments = [\"FI_PLI_FEWSHOT\"]\n",
    "experiments = [\"FI_PLI_FEWSHOT\", \"FI_PLI_FEWSHOT_DR_2024\", \"FI_PLI_FEWSHOT_DR_2025\", \"FI_PLI_FEWSHOT_DR_2026\", \"FI_PLI_FEWSHOT_DR_2027\", \"FI_PLI_FEWSHOT_DR_2028\", \"FI_PLI_FEWSHOT_DR_2029\", \"FI_PLI_FEWSHOT_DR_2030\", \"FI_PLI_FEWSHOT_DR_2031\", \"FI_PLI_FEWSHOT_DR_2032\"]\n",
    "models = [\"Meditron-70B-GPTQ\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, models=models, difficulty=\"dr_eval\", fields=[DIAG])\n",
    "\n",
    "for experiment in experiments:\n",
    "    model_scores = {}\n",
    "    model_scores[\"Meditron-70B-GPTQ\"] = experiment_scores[experiment][\"Meditron-70B-GPTQ\"]\n",
    "    # Replace keys with simplified names\n",
    "    model_scores = {prettify_model_name[name]: model_scores[name] for name in model_scores.keys()}\n",
    "\n",
    "    # Capitalize first letter of each disease\n",
    "    model_scores = {model: {disease.capitalize(): score for disease, score in category_dict[DIAG].items()} for model, category_dict in model_scores.items()}\n",
    "\n",
    "    # Calculate mean performance and add to disease dict\n",
    "    for category, disease_dict in model_scores.items():\n",
    "        disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))\n",
    "\n",
    "    # Collect data in a list for DataFrame creation\n",
    "    model_names = model_scores.keys()\n",
    "    for pathology in ['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean']:\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            score = model_scores[model_name][pathology]\n",
    "            data_to_plot.append({'Model': model_name, 'Performance': score, 'Pathology': pathology})\n",
    "\n",
    "\n",
    "experiments = [\"FI_PLI\"]\n",
    "experiments = [\"FI_PLI\", \"FI_PLI_DR_2024\", \"FI_PLI_DR_2025\", \"FI_PLI_DR_2026\", \"FI_PLI_DR_2027\", \"FI_PLI_DR_2028\", \"FI_PLI_DR_2029\", \"FI_PLI_DR_2030\", \"FI_PLI_DR_2031\", \"FI_PLI_DR_2032\",]\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\", \"ClinicalCamel-70B-GPTQ\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, models=models, difficulty=\"dr_eval\", fields=[DIAG])\n",
    "\n",
    "for experiment in experiments:\n",
    "    model_scores = experiment_scores[experiment]\n",
    "    # Replace keys with simplified names\n",
    "    model_scores = {prettify_model_name[name]: model_scores[name] for name in model_scores.keys()}\n",
    "\n",
    "    # Capitalize first letter of each disease\n",
    "    model_scores = {model: {disease.capitalize(): score for disease, score in category_dict[DIAG].items()} for model, category_dict in model_scores.items()}\n",
    "\n",
    "    # Calculate mean performance and add to disease dict\n",
    "    for category, disease_dict in model_scores.items():\n",
    "        disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))\n",
    "\n",
    "    # Collect data in a list for DataFrame creation\n",
    "    model_names = model_scores.keys()\n",
    "    for pathology in ['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean']:\n",
    "        for model_idx, model_name in enumerate(model_names):\n",
    "            score = model_scores[model_name][pathology]\n",
    "            data_to_plot.append({'Model': model_name, 'Performance': score, 'Pathology': pathology})\n",
    "\n",
    "# Create the DataFrame\n",
    "df_to_plot = pd.DataFrame(data_to_plot)\n",
    "\n",
    "# Multiply performance by 100 to get percentage\n",
    "df_to_plot['Performance'] = df_to_plot['Performance']*100\n",
    "\n",
    "# Dr Means\n",
    "accuracies['Mean'] = np.mean(list(accuracies.values()))\n",
    "df['Accuracy'] = df['Diagnosis'].eq(df['Correct Diagnosis'])\n",
    "\n",
    "\n",
    "# Check which patients were incorrectly diagnosed by all doctors\n",
    "results = {}\n",
    "\n",
    "for pathology in incorrect_ids:\n",
    "    id_to_doctors = {}\n",
    "    for idx, id in enumerate(incorrect_ids[pathology]):\n",
    "        id_to_doctors.setdefault(id, []).append(incorrect_dr[pathology][idx])\n",
    "    results[pathology] = id_to_doctors\n",
    "\n",
    "# Printing the required output\n",
    "for pathology, id_to_doctors in results.items():\n",
    "    for id, doctors in id_to_doctors.items():\n",
    "        output_line = f\"{pathology},{id},{','.join(doctors)}\"\n",
    "        print(output_line)\n",
    "\n",
    "# Calculate mean accuracy for each doctor for each correct diagnosis\n",
    "dr_stats = df.groupby(['Doctor','Correct Diagnosis']).agg(\n",
    "    Mean_Accuracy=('Accuracy', 'mean'),\n",
    "    Std_Err=('Accuracy', 'sem'),\n",
    ").reset_index()\n",
    "dr_stats = dr_stats[dr_stats['Correct Diagnosis'].isin(['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis'])]\n",
    "dr_stats[\"Mean_Accuracy\"] *= 100\n",
    "dr_stats[\"Std_Err\"] *= 100\n",
    "\n",
    "# Add mean of each dr\n",
    "mean_dr_stats = dr_stats.groupby(['Doctor']).agg(\n",
    "    Mean_Accuracy=('Mean_Accuracy', 'mean'),\n",
    "    Std_Err=('Std_Err', 'mean'),\n",
    ").reset_index()\n",
    "mean_dr_stats['Correct Diagnosis'] = 'Mean'\n",
    "dr_stats = pd.concat([dr_stats, mean_dr_stats])\n",
    "dr_stats['Model'] = 'Doctor'\n",
    "dr_stats.drop(columns=['Doctor', 'Std_Err'], inplace=True)\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "dr_stats.rename(columns={'Correct Diagnosis': 'Pathology', 'Mean_Accuracy': 'Performance'}, inplace=True)\n",
    "dr_stats['Model'] = 'Doctors'\n",
    "#df_to_plot['Std_Err'] = 0\n",
    "model_df = pd.concat([df_to_plot, dr_stats])\n",
    "\n",
    "# Set the style and scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Create the plot\n",
    "plt.figure(figsize=(14, 6))\n",
    "ax = sns.barplot(x=\"Pathology\", y=\"Performance\", hue=\"Model\", data=model_df, palette=color_map, errorbar=(\"ci\", 100), hue_order=[\"Llama 2 Chat\", \"OASST\", \"WizardLM\", \"Clinical Camel\", \"Meditron\", \"Doctors\"])\n",
    "\n",
    "# Calculate the number of unique pathologies and models\n",
    "num_pathologies = len(model_df['Pathology'].unique())\n",
    "num_models = len(model_df['Model'].unique())\n",
    "\n",
    "# Draw vertical dotted lines\n",
    "for i in range(num_pathologies - 1):\n",
    "    line_position = (i + 1) - 0.5\n",
    "    #plt.axvline(x=line_position, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "# Annotate bars with performance\n",
    "for i, p in enumerate(ax.patches):\n",
    "        # Doctor should be printed next to the bar because of error bar\n",
    "        if i > 24:\n",
    "            if p.get_height() > 0:\n",
    "                ax.annotate(format(p.get_height(), '.0f'), \n",
    "                    (p.get_x() + p.get_width() / 2. + 0.1, p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points',\n",
    "                    fontsize=14,\n",
    "                    )\n",
    "        else:\n",
    "            if p.get_height() > 99:\n",
    "                ax.annotate(format(p.get_height(), '.0f'), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points',\n",
    "                    fontsize=12,\n",
    "                    )\n",
    "            elif p.get_height() > 0:\n",
    "                ax.annotate(format(p.get_height(), '.0f'), \n",
    "                    (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                    ha = 'center', va = 'center', \n",
    "                    xytext = (0, 9), \n",
    "                    textcoords = 'offset points',\n",
    "                    fontsize=14,\n",
    "                    )\n",
    "\n",
    "ax.set_ylabel(\"Diagnostic Accuracy (%)\")\n",
    "ax.set_ylim(0, 100)\n",
    "ax.set_title(\"\", fontsize=30)\n",
    "ax.set_xlabel(\"\")\n",
    "\n",
    "# Set legend\n",
    "models = model_df['Model'].unique()\n",
    "plt.legend(bbox_to_anchor=(1., 1.18), ncol=len(models), frameon=False, fontsize=15)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.savefig(f\"Figures/LLMsWorseThanDoctors.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dr_grouped = df.groupby(['Doctor', 'Correct Diagnosis']).agg(\n",
    "    Mean_Accuracy=('Accuracy', 'mean'),\n",
    "    Std_Err=('Accuracy', 'sem'),\n",
    ").reset_index()\n",
    "primary = dr_grouped[dr_grouped[\"Correct Diagnosis\"].isin(['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Normal Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "scores_subset = {}\n",
    "models = list(model_scores.keys())+[\"Meditron\"]\n",
    "\n",
    "for model in models:\n",
    "    x = model_df[(model_df['Model'] == model) & (model_df[\"Pathology\"] != \"Mean\")]['Performance']\n",
    "    scores_subset[model] = x.values\n",
    "    print(model)\n",
    "    print(stats.shapiro(x))\n",
    "    print()\n",
    "\n",
    "for dr in ['Knauer', 'Huber', 'Vielhauer', 'Bhagat']:\n",
    "    x = primary[(primary['Doctor'] == dr)]['Mean_Accuracy']\n",
    "    scores_subset[dr] = x.values*100\n",
    "    scores_subset[\"Doctors\"] = scores_subset.get(\"Doctors\", []) + list(x.values*100)\n",
    "    print(dr)\n",
    "    print(stats.shapiro(x))\n",
    "    print()\n",
    "\n",
    "scores_subset[\"Doctors\"] = np.array(scores_subset[\"Doctors\"])\n",
    "print(\"Doctors\")\n",
    "print(stats.shapiro(scores_subset[\"Doctors\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check Equal Variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dr in ['Knauer', 'Huber', 'Vielhauer', 'Bhagat', 'Doctors']:\n",
    "    for model in models:\n",
    "        print(dr, model)\n",
    "        #print(stats.levene(scores_subset[dr], scores_subset[model], center='mean'))\n",
    "        print(stats.bartlett(scores_subset[dr], scores_subset[model]))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run T-test Unequal Variances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dr in ['Knauer', 'Huber', 'Vielhauer', 'Bhagat', 'Doctors']:\n",
    "    for model in models:\n",
    "        print(dr, model)\n",
    "        ttest_res = stats.ttest_ind(scores_subset[dr], scores_subset[model], equal_var=False)\n",
    "        print(ttest_res)\n",
    "        print(ttest_res.pvalue*5)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generalist vs Specialist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check variances\n",
    "\n",
    "for specialist_model in [\"Clinical Camel\", \"Meditron\"]:\n",
    "    for generalist_model in [\"Llama 2 Chat\", \"OASST\", \"WizardLM\"]:\n",
    "        print(specialist_model, generalist_model)\n",
    "        print(stats.levene(scores_subset[specialist_model], scores_subset[generalist_model], center='mean'))\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Mann whitney U test\n",
    "for specialist_model in [\"Clinical Camel\", \"Meditron\"]:\n",
    "    for generalist_model in [\"Llama 2 Chat\", \"OASST\", \"WizardLM\"]:\n",
    "        print(specialist_model, generalist_model)\n",
    "        #print(stats.mannwhitneyu(scores_subset[specialist_model], scores_subset[generalist_model], alternative='two-sided'))\n",
    "        ttest_res = stats.ttest_ind(scores_subset[specialist_model], scores_subset[generalist_model], equal_var=False)\n",
    "        #print(ttest_res)\n",
    "        print(ttest_res.pvalue*3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact No Abbrv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "experiment = 'FI_PLI_DR_NOABBR'\n",
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "fields = [DIAG]\n",
    "#models = [\"ClinicalCamel-70B-GPTQ\"]\n",
    "\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores([experiment], models=models, fields=fields, difficulty=\"original_dr_eval\")\n",
    "model_scores = experiment_scores[experiment]\n",
    "\n",
    "data = []\n",
    "for model in model_scores.keys():\n",
    "    mean_diagnosis = np.mean([model_scores[model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']])\n",
    "    for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "        data.append([model, patho.capitalize(), model_scores[model][DIAG][patho]])\n",
    "    data.append([model, 'Mean', mean_diagnosis])\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Model', 'Pathology', 'Diagnostic Accuracy'])\n",
    "df['Diagnostic Accuracy'] *= 100\n",
    "df['Model'] = df['Model'].apply(lambda x: prettify_model_name[x])\n",
    "\n",
    "# Reshaping the dataframe\n",
    "#melted_df = df.melt(id_vars=['Model'], var_name='Category', value_name='Diagnostic Accuracy' )\n",
    "\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating the bar plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "bar_plot = sns.barplot(x='Pathology', y='Diagnostic Accuracy', hue='Model', data=df, palette=color_map)\n",
    "\n",
    "# Calculate the number of unique pathologies and models\n",
    "num_pathologies = len(df['Pathology'].unique())\n",
    "num_models = len(df['Model'].unique())\n",
    "\n",
    "# Draw vertical dotted lines\n",
    "for i in range(num_pathologies - 1):\n",
    "    line_position = (i + 1) - 0.5\n",
    "    #plt.axvline(x=line_position, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.1f'), \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha = 'center', va = 'center', \n",
    "                      xytext = (0, 9), \n",
    "                      textcoords = 'offset points',\n",
    "                      fontsize=14,)\n",
    "\n",
    "\n",
    "# Additional plot formatting\n",
    "plt.title('')\n",
    "plt.ylabel('Diagnostic Accuracy (%)')\n",
    "plt.xlabel('')\n",
    "plt.ylim(0, 100)\n",
    "plt.legend(bbox_to_anchor=(0.85, 1.15),  ncol=len(model_scores.keys()), frameon=False, fontsize=16)\n",
    "plt.savefig(f\"Figures/DiagnosticAccuraciesFIDrNoAbbrv.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "fields = [DIAG]\n",
    "\n",
    "prettify_experiment_name = {\n",
    "    \"FI_PLI\": \"Vanilla Prompt\",\n",
    "    \"FI_PLI_NOSYSTEMNOUSER\": \"No System Or User Instruction\",\n",
    "    \"FI_PLI_NOMEDICAL\": \"No Medical Terminology\",\n",
    "    \"FI_PLI_NOSYSTEM\": \"No System Instruction\",\n",
    "    \"FI_PLI_MINIMALSYSTEM\": \"Minimal System Instruction\",\n",
    "    \"FI_PLI_SERIOUS\": \"Most Serious Final Diagnosis\",\n",
    "    \"FI_PLI_MAINDIAGNOSIS\": \"Main Diagnosis\", \n",
    "    \"FI_PLI_NOFINAL\": \"Diagnosis\",\n",
    "    \"FI_PLI_PRIMARYDIAGNOSIS\": \"Primary Diagnosis\",\n",
    "}\n",
    "\n",
    "experiments = [\"FI_PLI\", \"FI_PLI_NOSYSTEMNOUSER\", \"FI_PLI_NOMEDICAL\", \"FI_PLI_NOSYSTEM\", \"FI_PLI_MINIMALSYSTEM\", \"FI_PLI_NOFINAL\", \"FI_PLI_MAINDIAGNOSIS\", \"FI_PLI_PRIMARYDIAGNOSIS\", \"FI_PLI_SERIOUS\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=fields, models=models)\n",
    "\n",
    "# Save only differences\n",
    "data_to_plot = []\n",
    "for experiment in [\"FI_PLI_NOSYSTEMNOUSER\", \"FI_PLI_NOMEDICAL\", \"FI_PLI_NOSYSTEM\", \"FI_PLI_MINIMALSYSTEM\", \"FI_PLI_NOFINAL\", \"FI_PLI_MAINDIAGNOSIS\", \"FI_PLI_PRIMARYDIAGNOSIS\", \"FI_PLI_SERIOUS\"]:\n",
    "    for model in models:\n",
    "        for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "            data_to_plot.append({\n",
    "                'Experiment': experiment,\n",
    "                'Model': prettify_model_name[model],\n",
    "                'Pathology': patho.capitalize(),\n",
    "                'Diagnostic Accuracy': experiment_scores[experiment][model][DIAG][patho]- experiment_scores[\"FI_PLI\"][model][DIAG][patho],\n",
    "            })\n",
    "        # Add mean\n",
    "        data_to_plot.append({\n",
    "            'Experiment': experiment,\n",
    "            'Model': prettify_model_name[model],\n",
    "            'Pathology': 'Mean',\n",
    "            'Diagnostic Accuracy': np.mean([experiment_scores[experiment][model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']]) - np.mean([experiment_scores[\"FI_PLI\"][model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']]),\n",
    "        })\n",
    "df_differences = pd.DataFrame(data_to_plot)\n",
    "df_differences['Diagnostic Accuracy'] *= 100\n",
    "\n",
    "# Setting the font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.8)\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "df_differences['Experiment'] = df_differences['Experiment'].map(prettify_experiment_name)\n",
    "g = sns.catplot(x='Diagnostic Accuracy', y=\"Model\", hue=\"Pathology\", data=df_differences, kind=\"bar\", palette=color_map, col=\"Experiment\", legend_out=True, col_wrap=4)\n",
    "for ax in g.axes.flatten():\n",
    "    ax.set_xlabel(f\"Change in Diagnostic\\nAccuracy (%)\")\n",
    "\n",
    "    # Get unique models to determine the number of lines needed\n",
    "    unique_models = df_differences['Model'].unique()\n",
    "    for i in range(len(unique_models) - 1):\n",
    "        ax.axhline(y=i + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "g.set_titles(col_template=\"{col_name}\", size='large')\n",
    "\n",
    "# Adjusting layout and showing only one legend\n",
    "plt.tight_layout()\n",
    "\n",
    "g._legend.remove()\n",
    "plt.legend(bbox_to_anchor=(0.5, 2.55),  ncol=5, frameon=False, fontsize=24)\n",
    "#plt.title(\"Effect of Including Summarization on Diagnosing with Full Information\", fontsize=18)\n",
    "plt.savefig(f\"Figures/PromptFIDelta.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_differences[df_differences[\"Experiment\"]==\"Primary Diagnosis\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Only Abnormal Labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "fields = [DIAG]\n",
    "\n",
    "experiments = [\"FI_PLI\", \"FI_PLI_ONLYABNORMAL\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=fields, models=models)\n",
    "\n",
    "# Save only differences\n",
    "data_to_plot = []\n",
    "for model in models:\n",
    "    for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']:\n",
    "        data_to_plot.append({\n",
    "            'Model': prettify_model_name[model],\n",
    "            'Pathology': patho.capitalize(),\n",
    "            'Diagnostic Accuracy': experiment_scores[\"FI_PLI_ONLYABNORMAL\"][model][DIAG][patho] - experiment_scores[\"FI_PLI\"][model][DIAG][patho],\n",
    "        })\n",
    "    # Add mean\n",
    "    data_to_plot.append({\n",
    "        'Model': prettify_model_name[model],\n",
    "        'Pathology': 'Mean',\n",
    "        'Diagnostic Accuracy': np.mean([experiment_scores[\"FI_PLI_ONLYABNORMAL\"][model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']]) - np.mean([experiment_scores[\"FI_PLI\"][model][DIAG][patho] for patho in ['appendicitis', 'cholecystitis', 'diverticulitis', 'pancreatitis']]),\n",
    "    })\n",
    "df_differences = pd.DataFrame(data_to_plot)\n",
    "df_differences['Diagnostic Accuracy'] *= 100\n",
    "\n",
    "# Setting the font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "# Creating a combined plot\n",
    "fig, axes = plt.subplots(1, 1, figsize=(15, 7))\n",
    "\n",
    "barplot = sns.barplot(x='Diagnostic Accuracy', y=\"Model\", hue=\"Pathology\", data=df_differences, palette=color_map)\n",
    "barplot.set_xlabel(f\"Change in Diagnostic Accuracy (%)\")\n",
    "\n",
    "unique_models = df_differences['Model'].unique()\n",
    "for i in range(len(unique_models) - 1):\n",
    "    barplot.axhline(y=i + 0.5, color='gray', linestyle='--', linewidth=1)\n",
    "\n",
    "\n",
    "# Loop through the bars and add text labels\n",
    "for i, p in enumerate(barplot.patches):\n",
    "    if i > 14:\n",
    "        continue\n",
    "    width = p.get_width() # get bar length\n",
    "    if width > 0:\n",
    "        shift = 0.1\n",
    "    elif width < 0:\n",
    "        shift = -0.9\n",
    "    barplot.text(width + shift, # set the text at 0.1 unit right of the bar\n",
    "                 p.get_y() + p.get_height() / 2, # get Y coordinate + half of the bar's height\n",
    "                 '{:1.1f}'.format(width), # format the value\n",
    "                 ha = 'left',   # horizontal alignment\n",
    "                 va = 'center') # vertical alignment\n",
    "    \n",
    "\n",
    "# Adjusting layout and showing only one legend\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.legend(bbox_to_anchor=(1, 1.2),  ncol=5, frameon=False, fontsize=18)\n",
    "plt.title(\"Effect of Including Only Abnormal Laboratory Results on CDM-FI Diagnostic Accuracy\", fontsize=18)\n",
    "plt.savefig(f\"Figures/Abnormal.pdf\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single Modality Diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "experiments = [\"FI_PLI\", \"FI_H_PROBS\", \"FI_P_PROBS\", \"FI_L_PROBS\", \"FI_I_PROBS\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=[DIAG])\n",
    "experiment_names_map = {\n",
    "            \"FI_PLI\": \"All\",\n",
    "            \"FI_H_PROBS\": \"History of Present Illness (HPI)\",\n",
    "            \"FI_P_PROBS\": \"HPI + Physical Examination\",\n",
    "            \"FI_L_PROBS\": \"HPI + Laboratory Tests\",\n",
    "            \"FI_I_PROBS\": \"HPI + Imaging\",\n",
    "        }\n",
    "experiment_scores = {experiment_names_map[name]: experiment_scores[name] for name in experiment_scores.keys()}\n",
    "for model in [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", 'WizardLM-70B-V1.0-GPTQ']:\n",
    "    model_name = prettify_model_name[model]\n",
    "\n",
    "    # Capitalize first letter of each disease\n",
    "    experiment_scores = {experiment: {model: {category: {disease.capitalize(): score for disease, score in disease_dict.items()} for category, disease_dict in category_dict.items()} for model, category_dict in exp_dict.items()} for experiment, exp_dict in experiment_scores.items()}\n",
    "\n",
    "    # Calculate mean performance and add to disease dict\n",
    "    for experiment, category_dict in experiment_scores.items():\n",
    "        for category, disease_dict in category_dict[model].items():\n",
    "            disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))\n",
    "\n",
    "    model_scores = {train_type: {k: v for k, v in score_dict[model].items()} for train_type, score_dict in experiment_scores.items()}\n",
    "\n",
    "# Set the seaborn style and increase font sizes\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Set up the plot\n",
    "model_names = list(experiment_scores['All'].keys())\n",
    "pretty_model_names = [prettify_model_name[name] for name in model_names]\n",
    "pathologies = ['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean']\n",
    "\n",
    "# Collect data in a list for DataFrame creation\n",
    "data_to_plot = []\n",
    "for experiment in experiment_scores.keys():\n",
    "    model_names = list(experiment_scores[experiment].keys())\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        for pathology in pathologies:\n",
    "            score = experiment_scores[experiment][model_name][DIAG][pathology]\n",
    "            data_to_plot.append({'Experiment': experiment, 'Model': model_name, 'Performance': score, 'Pathology': pathology})\n",
    "\n",
    "# Create the DataFrame\n",
    "df_to_plot = pd.DataFrame(data_to_plot)\n",
    "df_to_plot['Model'] = df_to_plot['Model'].map(prettify_model_name)\n",
    "df_to_plot['Performance'] *= 100\n",
    "\n",
    "# Set up the plot with increased figure size\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "\n",
    "# Plot using Seaborn's boxplot for actual scores\n",
    "palette = sns.color_palette(\"muted\", n_colors=len(pathologies))  # A Seaborn color scheme\n",
    "# Filter out the data for the All experiment\n",
    "df_All = df_to_plot[df_to_plot['Experiment'] == 'All']\n",
    "df_other_experiments = df_to_plot[df_to_plot['Experiment'] != 'All']\n",
    "\n",
    "# First, create a catplot with the other experiments\n",
    "g = sns.catplot(x=\"Pathology\", y=\"Performance\", hue=\"Experiment\", col=\"Model\",\n",
    "                data=df_other_experiments, kind=\"strip\",\n",
    "                palette=palette, jitter=False,\n",
    "                height=5, aspect=1, s=80, legend_out=True)\n",
    "\n",
    "dt_to_max = pd.concat([df_All, df_other_experiments])\n",
    "df_grouped_max = dt_to_max.groupby(['Model', 'Pathology'])['Performance'].max().reset_index()\n",
    "df_grouped_max = df_grouped_max[df_grouped_max['Pathology'] != 'Mean']\n",
    "df_theoretical_best_performance = df_grouped_max.groupby('Model')['Performance'].mean()\n",
    "# Now, iterate over each subplot and add a line for All\n",
    "for ax, model in zip(g.axes.flatten(), df_to_plot['Model'].unique()):\n",
    "    # Get the All data for this model\n",
    "    model_All_data = df_All[df_All['Model'] == model]\n",
    "    # Find the x-axis positions for each category\n",
    "    x_positions = ax.get_xticks()\n",
    "    for i, pathology in enumerate(model_All_data['Pathology'].unique()):\n",
    "        # Find the performance for All for this pathology and model\n",
    "        performance_ALL = model_All_data[model_All_data['Pathology'] == pathology]['Performance'].item()\n",
    "        performance_theoretical_best = df_theoretical_best_performance[model].item()\n",
    "        # Draw the line on the relevant subplot within the category x-axis limits\n",
    "        ax.hlines(performance_ALL, x_positions[i] - 0.2, x_positions[i] + 0.2, color='black', linestyle='-', lw=2, label='All Information' if i == 0 else \"\")\n",
    "        if pathology == 'Mean':\n",
    "            ax.hlines(performance_theoretical_best, x_positions[i] - 0.2, x_positions[i] + 0.2, color='red', linestyle='-', lw=3, label='Theoretical Best')\n",
    "\n",
    "        # Find the maximum performance difference for each pathology\n",
    "        model_data = df_other_experiments[(df_other_experiments['Model'] == model) & (df_other_experiments['Pathology'] == pathology)]\n",
    "        max_diff = 0\n",
    "        max_performance = 0\n",
    "        for j, row in model_data.iterrows():\n",
    "            difference = row['Performance'] - performance_ALL\n",
    "            if difference > max_diff:\n",
    "                max_diff = difference\n",
    "                max_performance = row['Performance']\n",
    "        if pathology == 'Mean':\n",
    "            difference = performance_theoretical_best - performance_ALL\n",
    "            if difference > max_diff:\n",
    "                max_diff = difference\n",
    "                max_performance = performance_theoretical_best\n",
    "\n",
    "        # Plot arrow and text for the maximum difference\n",
    "        if max_diff > 0:\n",
    "            extra_x = 0\n",
    "            extra_y = 0\n",
    "            if pathology == \"Mean\":\n",
    "                extra_x = -0.2\n",
    "                extra_y = 4\n",
    "            if pathology == \"Pancreatitis\" and model == \"Llama 2 Chat\":\n",
    "                extra_y = 4\n",
    "                extra_x = -0.1\n",
    "            ax.annotate('', xy=(x_positions[i]+0.2, performance_ALL), xytext=(x_positions[i]+0.2, max_performance+2),\n",
    "                        arrowprops=dict(color='black', arrowstyle='<|-'), ha='center')\n",
    "            ax.text(x_positions[i] + 0.25 + extra_x, max_performance + extra_y, f'+{max_diff:.1f}', verticalalignment='center')\n",
    "\n",
    "\n",
    "\n",
    "for ax in g.axes.flatten():\n",
    "    # Rotate the x-axis tick labels\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45)\n",
    "    ax.tick_params(axis='x', labelsize='large')\n",
    "    ax.tick_params(axis='y', labelsize='large')\n",
    "    ax.set_xlabel('') \n",
    "    ax.set_ylabel('Diagnostic Accuracy (%)', fontsize='x-large')\n",
    "    ax.set_ylim(0, 100)\n",
    "g.set_titles(col_template=\"{col_name}\", size='large')\n",
    "\n",
    "\n",
    "g._legend.remove()\n",
    "plt.legend(bbox_to_anchor=(0.7, 1.35),  ncol=3, frameon=False, fontsize=16)\n",
    "\n",
    "plt.savefig(f\"Figures/InformationOverload.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_theoretical_best_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modality Order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "experiments = [\"FI_PLI\", \"FI_PIL\", \"FI_LIP\", \"FI_LPI\", \"FI_IPL\", \"FI_ILP\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=[DIAG])\n",
    "\n",
    "for model in [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", 'WizardLM-70B-V1.0-GPTQ']:\n",
    "    model_name = prettify_model_name[model]\n",
    "\n",
    "    # Capitalize first letter of each disease\n",
    "    experiment_scores = {experiment: {model: {category: {disease.capitalize(): score for disease, score in disease_dict.items()} for category, disease_dict in category_dict.items()} for model, category_dict in exp_dict.items()} for experiment, exp_dict in experiment_scores.items()}\n",
    "\n",
    "    # Calculate mean performance and add to disease dict\n",
    "    for experiment, category_dict in experiment_scores.items():\n",
    "        for category, disease_dict in category_dict[model].items():\n",
    "            disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))\n",
    "\n",
    "    model_scores = {train_type: {k: v for k, v in score_dict[model].items()} for train_type, score_dict in experiment_scores.items()}\n",
    "\n",
    "    model_names_map = {\n",
    "        \"FI_PLI\": \"Physical, Lab, Imaging\",\n",
    "        \"FI_PIL\": \"Physical, Imaging, Lab\",\n",
    "        \"FI_LIP\": \"Lab, Imaging, Physical\",\n",
    "        \"FI_LPI\": \"Lab, Physical, Imaging\",\n",
    "        \"FI_IPL\": \"Imaging, Physical, Lab\",\n",
    "        \"FI_ILP\": \"Imaging, Lab, Physical\",\n",
    "    }\n",
    "    # Replace keys with simplified names\n",
    "    model_scores = {model_names_map[name]: model_scores[name][DIAG] for name in model_scores.keys()}\n",
    "\n",
    "    generate_latex_tables_full_info(model_scores, model_name)\n",
    "\n",
    "# Set the seaborn style and increase font sizes\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Set up the plot\n",
    "model_names = list(experiment_scores['FI_PLI'].keys())\n",
    "pretty_model_names = [prettify_model_name[name] for name in model_names]\n",
    "pathologies = ['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean']\n",
    "\n",
    "# Collect data in a list for DataFrame creation\n",
    "data_to_plot = []\n",
    "for experiment in experiment_scores.keys():\n",
    "    model_names = list(experiment_scores[experiment].keys())\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        for pathology in pathologies:\n",
    "            score = experiment_scores[experiment][model_name][DIAG][pathology]\n",
    "            data_to_plot.append({'Experiment': experiment, 'Model': model_name, 'Performance': score, 'Pathology': pathology})\n",
    "\n",
    "# Create the DataFrame\n",
    "df_to_plot = pd.DataFrame(data_to_plot)\n",
    "df_to_plot[\"Performance\"] *= 100\n",
    "\n",
    "# Calculate range of each model and pathology combination\n",
    "range_df = (df_to_plot.groupby(['Model', 'Pathology'])['Performance'].max()-df_to_plot.groupby(['Model', 'Pathology'])['Performance'].min()).reset_index()\n",
    "\n",
    "# Recalc mean\n",
    "range_df = range_df[range_df['Pathology'] != 'Mean']\n",
    "range_df_mean = range_df.groupby('Model')['Performance'].mean().reset_index()\n",
    "range_df_mean['Pathology'] = 'Mean'\n",
    "range_df = pd.concat([range_df, range_df_mean])\n",
    "\n",
    "# Set up the plot with increased figure size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "\n",
    "# Plot using Seaborn's boxplot for actual scores\n",
    "bar_plot = sns.barplot(x=\"Model\", y=\"Performance\", hue=\"Pathology\", data=range_df, palette=color_map, hue_order=['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean'])\n",
    "\n",
    "num_pathologies = len(pathologies)\n",
    "num_models = len(model_names)\n",
    "\n",
    "# Draw vertical dotted lines\n",
    "for i in range(num_models - 1):\n",
    "    line_position = (i + 1) - 0.5\n",
    "    #plt.axvline(x=line_position, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "# Adding the scores above the bars\n",
    "for p in bar_plot.patches:\n",
    "    if p.get_height() > 0:\n",
    "        bar_plot.annotate(format(p.get_height(), '.1f'), \n",
    "                      (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                      ha = 'center', va = 'center', \n",
    "                      xytext = (0, 9), \n",
    "                      textcoords = 'offset points',\n",
    "                      fontsize=14,)\n",
    "\n",
    "# Adjust legend to only show unique pathologies and increase font size\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "unique_labels = dict(zip(labels, handles))\n",
    "\n",
    "# Improve plot aesthetics\n",
    "plt.xticks(range(len(model_names)), pretty_model_names, fontsize='large')\n",
    "plt.yticks(fontsize='large')\n",
    "plt.ylim(0, 30)\n",
    "plt.xlabel('Model', fontsize='x-large')\n",
    "plt.ylabel('Change in Diagnostic Accuracy (%)', fontsize='large')\n",
    "plt.title('Changing the Order of Information Changes Diagnostic Accuracy', fontsize='x-large', y=1.03)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.3),  ncol=len(pathologies), frameon=False, fontsize='large')\n",
    "\n",
    "# Ensure everything fits without overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# SAVE\n",
    "plt.savefig(f\"Figures/SensitiveToOrder.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "experiments = [\"FI_PLI\", \"FI_PIL\", \"FI_LIP\", \"FI_LPI\", \"FI_IPL\", \"FI_ILP\"]\n",
    "experiment_results, experiment_evals, experiment_scores = load_scores(experiments, fields=[DIAG])\n",
    "\n",
    "for model in [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", 'WizardLM-70B-V1.0-GPTQ']:\n",
    "    model_name = prettify_model_name[model]\n",
    "\n",
    "    # Capitalize first letter of each disease\n",
    "    experiment_scores = {experiment: {model: {category: {disease.capitalize(): score for disease, score in disease_dict.items()} for category, disease_dict in category_dict.items()} for model, category_dict in exp_dict.items()} for experiment, exp_dict in experiment_scores.items()}\n",
    "\n",
    "    # Calculate mean performance and add to disease dict\n",
    "    for experiment, category_dict in experiment_scores.items():\n",
    "        for category, disease_dict in category_dict[model].items():\n",
    "            disease_dict[\"Mean\"] = np.mean(list(disease_dict.values()))\n",
    "\n",
    "    model_scores = {train_type: {k: v for k, v in score_dict[model].items()} for train_type, score_dict in experiment_scores.items()}\n",
    "\n",
    "    model_names_map = {\n",
    "        \"FI_PLI\": \"Physical, Lab, Imaging\",\n",
    "        \"FI_PIL\": \"Physical, Imaging, Lab\",\n",
    "        \"FI_LIP\": \"Lab, Imaging, Physical\",\n",
    "        \"FI_LPI\": \"Lab, Physical, Imaging\",\n",
    "        \"FI_IPL\": \"Imaging, Physical, Lab\",\n",
    "        \"FI_ILP\": \"Imaging, Lab, Physical\",\n",
    "    }\n",
    "    # Replace keys with simplified names\n",
    "    model_scores = {model_names_map[name]: model_scores[name][DIAG] for name in model_scores.keys()}\n",
    "\n",
    "    #generate_latex_tables_full_info(model_scores, model_name)\n",
    "\n",
    "# Set the seaborn style and increase font sizes\n",
    "sns.set(style=\"whitegrid\", font_scale=1.2)\n",
    "\n",
    "# Set up the plot\n",
    "model_names = list(experiment_scores['FI_PLI'].keys())\n",
    "pretty_model_names = [prettify_model_name[name] for name in model_names]\n",
    "pathologies = ['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean']\n",
    "\n",
    "# Collect data in a list for DataFrame creation\n",
    "data_to_plot = []\n",
    "for experiment in experiment_scores.keys():\n",
    "    model_names = list(experiment_scores[experiment].keys())\n",
    "    for model_idx, model_name in enumerate(model_names):\n",
    "        for pathology in pathologies:\n",
    "            score = experiment_scores[experiment][model_name][DIAG][pathology]\n",
    "            data_to_plot.append({'Experiment': experiment, 'Model': model_name, 'Performance': score, 'Pathology': pathology})\n",
    "\n",
    "# Create the DataFrame\n",
    "df_to_plot = pd.DataFrame(data_to_plot)\n",
    "df_to_plot[\"Performance\"] *= 100\n",
    "\n",
    "# Set up the plot with increased figure size\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "\n",
    "# Plot using Seaborn's boxplot for actual scores\n",
    "boxplot = sns.boxplot(x=\"Model\", y=\"Performance\", hue=\"Pathology\", data=df_to_plot, palette=color_map)\n",
    "ax = plt.gca()\n",
    "\n",
    "num_pathologies = len(pathologies)\n",
    "num_models = len(model_names)\n",
    "num_patches_per_box = 6\n",
    "\n",
    "for i in range(0, len(ax.lines), num_patches_per_box):\n",
    "    box = ax.patches[i // num_patches_per_box]\n",
    "    # Get the data for this particular box\n",
    "    pathology_index = (i // num_patches_per_box) // num_models\n",
    "    model_index = (i // num_patches_per_box) % num_models\n",
    "    model_name = model_names[model_index]\n",
    "    pathology = pathologies[pathology_index]\n",
    "    group_data = df_to_plot[(df_to_plot['Model'] == model_name) & (df_to_plot['Pathology'] == pathology)]['Performance']\n",
    "\n",
    "    # Calculate the max and min of the group\n",
    "    max_value = group_data.max()\n",
    "    min_value = group_data.min()\n",
    "    whisker_diff = max_value - min_value\n",
    "    \n",
    "    # Calculate the location to place the text: above the upper whisker\n",
    "    whisker = ax.lines[i+1]  # upper whisker line\n",
    "    x, y = (whisker.get_xdata()[0], whisker.get_ydata()[1])\n",
    "    \n",
    "    # Add text annotation above each whisker with the whisker difference\n",
    "    ax.text(x, y + (0.02 * (ax.get_ylim()[1] - ax.get_ylim()[0])), f'{whisker_diff:.2f}',\n",
    "             horizontalalignment='center', size='small', color='black', weight='semibold')\n",
    "\n",
    "\n",
    "# Adjust legend to only show unique pathologies and increase font size\n",
    "handles, labels = plt.gca().get_legend_handles_labels()\n",
    "unique_labels = dict(zip(labels, handles))\n",
    "\n",
    "# Add vertical dashed lines in between the models\n",
    "for i in range(len(model_names) - 1):\n",
    "    plt.axvline(x=i + 0.5, color='grey', linestyle='--', linewidth=1)\n",
    "\n",
    "# Improve plot aesthetics\n",
    "plt.xticks(range(len(model_names)), pretty_model_names, fontsize='large')\n",
    "plt.yticks(fontsize='large')\n",
    "plt.ylim(0, 100)\n",
    "plt.xlabel('Model', fontsize='x-large')\n",
    "plt.ylabel('Diagnostic Accuracy (%)', fontsize='x-large')\n",
    "plt.title('Changing the Order of Information Changes Diagnostic Accuracy', fontsize='x-large', y=1.03)\n",
    "plt.legend(bbox_to_anchor=(1.0, 1.3),  ncol=len(pathologies), frameon=False, fontsize='large')\n",
    "\n",
    "# Ensure everything fits without overlapping\n",
    "plt.tight_layout()\n",
    "\n",
    "# SAVE\n",
    "plt.savefig(f\"Figures/SensitiveToOrder_boxplot.pdf\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming df_to_plot is your existing DataFrame\n",
    "models = df_to_plot['Model'].unique()\n",
    "\n",
    "experiment_mapping = {\n",
    "    \"FI_PLI\": \"Physical, Lab, Imaging\",\n",
    "    \"FI_PIL\": \"Physical, Imaging, Lab\",\n",
    "    \"FI_LIP\": \"Lab, Imaging, Physical\",\n",
    "    \"FI_LPI\": \"Lab, Physical, Imaging\",\n",
    "    \"FI_IPL\": \"Imaging, Physical, Lab\",\n",
    "    \"FI_ILP\": \"Imaging, Lab, Physical\",\n",
    "}\n",
    "\n",
    "# Function to add newline in column names after the second comma\n",
    "def format_column_name(name):\n",
    "    parts = name.split(', ')\n",
    "    if len(parts) > 2:\n",
    "        return '\\\\thead{' + ', '.join(parts[:2]) + '\\\\\\\\' + ', '.join(parts[2:]) + '}'\n",
    "    else:\n",
    "        return '\\\\thead{' + name + '}'\n",
    "\n",
    "latex_tables = {}\n",
    "for model in models:\n",
    "    # Pivot the DataFrame\n",
    "    pivoted_df = df_to_plot[df_to_plot['Model'] == model].pivot(index='Pathology', columns='Experiment', values='Performance')\n",
    "    \n",
    "    # Rename and format columns\n",
    "    formatted_columns = {col: format_column_name(experiment_mapping[col]) for col in pivoted_df.columns}\n",
    "    pivoted_df.rename(columns=formatted_columns, inplace=True)\n",
    "\n",
    "    # Move row 'Mean' to the end\n",
    "    pivoted_df = pivoted_df.reindex(['Appendicitis', 'Cholecystitis', 'Diverticulitis', 'Pancreatitis', 'Mean'])\n",
    "\n",
    "    # Bold the max value in each row\n",
    "    for row in pivoted_df.index:\n",
    "        max_value = pivoted_df.loc[row].max()\n",
    "        pivoted_df.loc[row] = pivoted_df.loc[row].apply(lambda x: f\"\\\\textbf{{{x:.2f}}}\" if x == max_value else f\"{x:.2f}\")\n",
    "\n",
    "    # Export to LaTeX\n",
    "    latex_tables[model] = pivoted_df.to_latex(escape=False, column_format='ccccccc')\n",
    "\n",
    "# latex_tables will contain the LaTeX strings for each model\n",
    "for model in models:\n",
    "    print(latex_tables[model])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lab Test Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiments = [\"RRTEST_FEWSHOT\", \"RRTEST\"]\n",
    "for experiment in experiments:\n",
    "    models = [\"Llama-2-70B-chat-GPTQ\", \"Llama2-70B-OASST-SFT-v10-GPTQ\", \"WizardLM-70B-V1.0-GPTQ\"]\n",
    "    data_to_plot = []\n",
    "    for model in models:\n",
    "        results_path = f\"logs/SOTA/{experiment}/{model}_*_{experiment}_results.pkl\"\n",
    "        results = []\n",
    "        for r in read_from_pickle_file(glob.glob(results_path)[0]):\n",
    "            results.append(r)\n",
    "\n",
    "        low_preds = []\n",
    "        normal_preds = []\n",
    "        high_preds = []\n",
    "        for r in results:\n",
    "            for res in r:\n",
    "                pred = res['Result']\n",
    "                gt = res['GT']\n",
    "                if gt == 'Low':\n",
    "                    low_preds.append(pred)\n",
    "                elif gt == 'Normal':\n",
    "                    normal_preds.append(pred)\n",
    "                elif gt == 'High':\n",
    "                    high_preds.append(pred)\n",
    "\n",
    "        acc_low = sum([1 if pred == 'Low' else 0 for pred in low_preds]) / len(low_preds)\n",
    "        acc_normal = sum([1 if pred == 'Normal' else 0 for pred in normal_preds]) / len(normal_preds)\n",
    "        acc_high = sum([1 if pred == 'High' else 0 for pred in high_preds]) / len(high_preds)\n",
    "\n",
    "        data_to_plot.append({\n",
    "            'Model': model,\n",
    "            'Lab Test Result': 'Low',\n",
    "            'Accuracy': acc_low,\n",
    "        })\n",
    "        data_to_plot.append({\n",
    "            'Model': model,\n",
    "            'Lab Test Result': 'Normal',\n",
    "            'Accuracy': acc_normal,\n",
    "        })\n",
    "        data_to_plot.append({\n",
    "            'Model': model,\n",
    "            'Lab Test Result': 'High',\n",
    "            'Accuracy': acc_high,\n",
    "        })\n",
    "    df_to_plot = pd.DataFrame(data_to_plot)\n",
    "\n",
    "    sns.set(style=\"whitegrid\", font_scale=1.4)\n",
    "\n",
    "    # Creating the bar plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    df_to_plot['Model'] = df_to_plot['Model'].map(prettify_model_name)\n",
    "    df_to_plot['Accuracy'] *= 100\n",
    "    bar_plot = sns.barplot(x='Lab Test Result', y='Accuracy', hue='Model', data=df_to_plot, palette=color_map)\n",
    "\n",
    "    # Adjust xtick fontsize\n",
    "    plt.xticks(fontsize=18)\n",
    "\n",
    "    # Adding the scores above the bars\n",
    "    for p in bar_plot.patches:\n",
    "        if p.get_height() > 0:\n",
    "            bar_plot.annotate(format(p.get_height(), '.2f'), \n",
    "                        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                        ha = 'center', va = 'center', \n",
    "                        xytext = (0, 9), \n",
    "                        textcoords = 'offset points',\n",
    "                        fontsize=16,)\n",
    "\n",
    "    # Calculate counts for each 'Lab Test Result'\n",
    "    counts = {\n",
    "        \"Low\": len(low_preds),\n",
    "        \"Normal\": len(normal_preds),\n",
    "        \"High\": len(high_preds),\n",
    "    }\n",
    "    # Annotate counts for each 'Lab Test Result'\n",
    "    for index, count in counts.items():\n",
    "        plt.text(x=index, y=-0.10, s=f'n = {count}', ha='center', va='center', fontsize=18, transform=bar_plot.get_xaxis_transform())\n",
    "\n",
    "\n",
    "    # Additional plot formatting\n",
    "    plt.title('')\n",
    "    plt.ylabel('Accuracy (%)', fontsize=22)\n",
    "    plt.xlabel('')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.legend(bbox_to_anchor=(0.9, 1.15),  ncol=len(model_scores.keys()), frameon=False, fontsize=18)\n",
    "    plt.savefig(f\"Figures/LabTest{experiment}.pdf\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
